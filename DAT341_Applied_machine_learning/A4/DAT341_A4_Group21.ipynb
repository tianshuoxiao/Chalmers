{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bd0abd",
   "metadata": {},
   "source": [
    "# DAT341 Programming assignment 4: Implementing linear classifiers\n",
    "## Yahui Wu(15hrs)\n",
    "\n",
    "\n",
    "## Tianshuo Xiao(15hrs)\n",
    "\n",
    "## Group 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe8b17",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "101e970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy (Gothenburg): 1.0\n",
      "Perceptron accuracy (Sydney): 0.5\n",
      "LinearSVC accuracy (Sydney): 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.linalg import blas\n",
    "\n",
    "X1 = [{'city':'Gothenburg', 'month':'July'},\n",
    "      {'city':'Gothenburg', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y1 = ['rain', 'rain', 'sun', 'rain']\n",
    "\n",
    "X2 = [{'city':'Sydney', 'month':'July'},\n",
    "      {'city':'Sydney', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y2 = ['rain', 'sun', 'sun', 'rain']\n",
    "\n",
    "classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier1.fit(X1, Y1)\n",
    "guesses1 = classifier1.predict(X1)\n",
    "print(f'Perceptron accuracy (Gothenburg): {accuracy_score(Y1, guesses1)}')\n",
    "\n",
    "\n",
    "classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "#classifier2 = make_pipeline(DictVectorizer(), LinearSVC())\n",
    "classifier2.fit(X2, Y2)\n",
    "guesses2 = classifier2.predict(X2)\n",
    "print(f'Perceptron accuracy (Sydney): {accuracy_score(Y2, guesses2)}')\n",
    "\n",
    "classifier3 = make_pipeline(DictVectorizer(), LinearSVC())\n",
    "classifier3.fit(X2, Y2)\n",
    "guesses3 = classifier3.predict(X2)\n",
    "print(f'LinearSVC accuracy (Sydney): {accuracy_score(Y2, guesses2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4314b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dc = DictVectorizer()\n",
    "x_1 = dc.fit_transform(X1)\n",
    "df = pd.DataFrame(x_1.toarray(),dtype='int8')\n",
    "df['label'] = Y1\n",
    "\n",
    "x_2 = dc.fit_transform(X2)\n",
    "df2 = pd.DataFrame(x_2.toarray(),dtype='int8')\n",
    "df2['label'] = Y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d7281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "\n",
    "X = pca.fit_transform(df.iloc[:,:-1].values)\n",
    "X2 = pca.fit_transform(df2.iloc[:,:-1].values)\n",
    "\n",
    "pos=pd.DataFrame()\n",
    "pos2=pd.DataFrame()\n",
    "\n",
    "pos['X'] =X[:, 0]\n",
    "pos2['X2'] =X[:, 0]\n",
    "\n",
    "pos['Y'] =X[:, 1]\n",
    "pos2['Y2'] =X[:, 1]\n",
    "\n",
    "pos['label'] = df['label']\n",
    "pos2['label'] = df2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "358b9dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "ax = pos.loc[pos['label']=='sun']\n",
    "\n",
    "X_1 = list(ax['X'])\n",
    "Y_1 = list(ax['Y'])\n",
    "plt.scatter(X_1,Y_1,s=10,c='r',label='sun')\n",
    "ax_0 = pos.loc[pos['label']=='rain']\n",
    "X_0 = list(ax_0['X'])\n",
    "Y_0 = list(ax_0['Y'])\n",
    "plt.scatter(X_0,Y_0,s=10,c='b',label='rain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d0893",
   "metadata": {},
   "source": [
    "We can see from the figure that the dataset can linearly separable. Therefore, the perceptron learning algorithm will find a separated w in a limited number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8f8ab0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pos2.loc[pos2['label']=='sun']\n",
    "\n",
    "X_1 = list(ax['X2'])\n",
    "Y_1 = list(ax['Y2'])\n",
    "plt.scatter(X_1,Y_1,s=10,c='r',label='sun')\n",
    "ax_0 = pos2.loc[pos2['label']=='rain']\n",
    "X_0 = list(ax_0['X2'])\n",
    "Y_0 = list(ax_0['Y2'])\n",
    "plt.scatter(X_0,Y_0,s=10,c='b',label='rain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f48cd",
   "metadata": {},
   "source": [
    "However, we can see from the figure that the data are not linearly separable, so there is not a w for classification.So this could be the reason for the misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d92120",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ba523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file shows a couple of implementations of the perceptron learning\n",
    "algorithm. It is based on the code from Lecture 3, but using the slightly\n",
    "more compact perceptron formulation that we saw in Lecture 6.\n",
    "\n",
    "There are two versions: Perceptron, which uses normal NumPy vectors and\n",
    "matrices, and SparsePerceptron, which uses sparse vectors and matrices.\n",
    "The latter may be faster when we have high-dimensional feature representations\n",
    "with a lot of zeros, such as when we are using a \"bag of words\" representation\n",
    "of documents.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "\n",
    "class Perceptron(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 0:\n",
    "                    self.w += y*x\n",
    "\n",
    "\n",
    "##### The following part is for the optional task.\n",
    "\n",
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SparsePerceptron(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 0:\n",
    "                    # (This corresponds to self.w += y*x above.)\n",
    "                    add_sparse_to_dense(x, self.w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4eedf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.76 sec.\n",
      "Accuracy: 0.7923.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        Perceptron()  \n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04dbe8",
   "metadata": {},
   "source": [
    "### Implementing the SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b697b",
   "metadata": {},
   "source": [
    "We implement the Pegasos algorithm for training support vector classifiers by converting the pseudocode in Algorithm 1 in the clarification document, which uses the loss function of hinge loss.. As for regularization parameter C and the number of training steps, we used the recommendations from the assignment. We iterated 20 times through the training set and set C to 1/N, where N is the number of instances in X_train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4bd1972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, C=1):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.C = C\n",
    "        self.loss = []\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        t = 1\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            #num_pairs = X.shape[0]\n",
    "            #np.random.shuffle(training_pairs)\n",
    "            \n",
    "            \n",
    "            for x, y in zip(X, Ye):\n",
    "                #t is a “counter” that increases by one for each training instance we process\n",
    "                t += 1\n",
    "                # Setting the step length\n",
    "                eta = 1/(self.C*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "                self.loss.append(max(0,1 - y*score))\n",
    "                \n",
    "                # If there was an error, update the weights by the hinge loss algorithm\n",
    "                if y*score < 1:\n",
    "                    self.w = (1-eta*self.C)*self.w + (eta*y)*x\n",
    "                else:\n",
    "                    self.w = (1-eta*self.C)*self.w\n",
    "                \n",
    "                \n",
    "                \n",
    "            avg_loss = sum(self.loss)/len(self.loss)\n",
    "            print(f\"Iteration {i}, average loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b5e667bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, average loss: 1.0553166752607162\n",
      "Iteration 1, average loss: 0.7135354237097752\n",
      "Iteration 2, average loss: 0.5898683868406869\n",
      "Iteration 3, average loss: 0.524517263262039\n",
      "Iteration 4, average loss: 0.4836368264658004\n",
      "Iteration 5, average loss: 0.45567450115529495\n",
      "Iteration 6, average loss: 0.43511272834389886\n",
      "Iteration 7, average loss: 0.4193510354118954\n",
      "Iteration 8, average loss: 0.4068605736600731\n",
      "Iteration 9, average loss: 0.3966681261190678\n",
      "Iteration 10, average loss: 0.3881894931124227\n",
      "Iteration 11, average loss: 0.38104028497834314\n",
      "Iteration 12, average loss: 0.374918366630379\n",
      "Iteration 13, average loss: 0.3696005213980102\n",
      "Iteration 14, average loss: 0.3649399704179835\n",
      "Iteration 15, average loss: 0.36083228996908523\n",
      "Iteration 16, average loss: 0.3571614196511262\n",
      "Iteration 17, average loss: 0.3538656161683418\n",
      "Iteration 18, average loss: 0.35089299222846493\n",
      "Iteration 19, average loss: 0.34819634207504246\n",
      "Training time: 8.78 sec.\n",
      "Accuracy: 0.8326.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X_svc, Y_svc = read_data('all_sentiment_shuffled.txt')\n",
    "    \n",
    "# Split into training and test parts.\n",
    "Xtrain_svc, Xtest_svc, Ytrain_svc, Ytest_svc = train_test_split(X_svc, Y_svc, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline_svc = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our LinearSVC, not sklearn.linear_model.LinearSVC\n",
    "        LinearSVC(n_iter=20, C=1/len(Xtrain_svc))  \n",
    "    )\n",
    "\n",
    "# Train the classifier.\n",
    "t0_svc = time.time()\n",
    "pipeline_svc.fit(Xtrain_svc, Ytrain_svc)\n",
    "t1_svc = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1_svc-t0_svc))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess_svc = pipeline_svc.predict(Xtest_svc)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest_svc, Yguess_svc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525fad9",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d423284",
   "metadata": {},
   "source": [
    "We converted the loss function to a log loss, with other parameters unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6c6f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, C=1):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.C = C\n",
    "        self.loss = []\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        t = 1\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            for x, y in zip(X, Ye):\n",
    "                t += 1\n",
    "                eta = 1/(self.C*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "                self.loss.append(np.log(1 + np.exp(-y * score)))\n",
    "                \n",
    "                # Update the weights by the log loss algorithm\n",
    "                self.w = (1-eta*self.C)*self.w + eta*(y*x)/(1+np.exp(y*(self.w*x))) \n",
    "                \n",
    "                \n",
    "        \n",
    "                                 \n",
    "            avg_loss = sum(self.loss)/len(self.loss)\n",
    "            print(f\"Iteration {i}, average loss: {avg_loss}\")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3be44f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, average loss: 0.9099698734725881\n",
      "Iteration 1, average loss: 0.677264840612741\n",
      "Iteration 2, average loss: 0.588659132737918\n",
      "Iteration 3, average loss: 0.5406165093688889\n",
      "Iteration 4, average loss: 0.5101184320693263\n",
      "Iteration 5, average loss: 0.4889053967464099\n",
      "Iteration 6, average loss: 0.473236189082661\n",
      "Iteration 7, average loss: 0.46115674445479543\n",
      "Iteration 8, average loss: 0.4515419326843679\n",
      "Iteration 9, average loss: 0.44369603605243496\n",
      "Iteration 10, average loss: 0.43716473306016723\n",
      "Iteration 11, average loss: 0.4316382446568743\n",
      "Iteration 12, average loss: 0.4268978047978385\n",
      "Iteration 13, average loss: 0.422784358645719\n",
      "Iteration 14, average loss: 0.41917939095346785\n",
      "Iteration 15, average loss: 0.4159927199417426\n",
      "Iteration 16, average loss: 0.4131544656795019\n",
      "Iteration 17, average loss: 0.4106096131799976\n",
      "Iteration 18, average loss: 0.40831423986130294\n",
      "Iteration 19, average loss: 0.40623284036087826\n",
      "Training time: 14.87 sec.\n",
      "Accuracy: 0.8065.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline_log = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our LinearSVC, not sklearn.linear_model.LinearSVC\n",
    "        LogisticRegression(n_iter=20, C=1/len(Xtrain_svc))  \n",
    "    )\n",
    "\n",
    "# Train the classifier.\n",
    "t0_log = time.time()\n",
    "pipeline_log.fit(Xtrain_svc, Ytrain_svc)\n",
    "t1_log = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1_log-t0_log))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess_svc = pipeline_log.predict(Xtest_svc)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest_svc, Yguess_svc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7361369",
   "metadata": {},
   "source": [
    "By comparing the results, we found that the performance of SVC is slightly higher than that of logistic regression. The accuracy of SVC is 0.8326 and logistic regression is 0.8065, which is 0.03 higher. Moreover, the running time of logistic regression is longer than that of SVC, 8.78s and 14.87s, separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f8689",
   "metadata": {},
   "source": [
    "## Optional task: Printing the value of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3e32e",
   "metadata": {},
   "source": [
    "We have implemented in the above program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca08ed",
   "metadata": {},
   "source": [
    "## Bonus tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb808a",
   "metadata": {},
   "source": [
    "### (a) Faster linear algebra operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "58e0ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastLinearSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, C=1):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.C = C\n",
    "        self.loss = []\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        t = 1\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            #num_pairs = X.shape[0]\n",
    "            #np.random.shuffle(training_pairs)\n",
    "            \n",
    "            \n",
    "            for x, y in zip(X, Ye):\n",
    "                t += 1\n",
    "                eta = 1/(self.C*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = blas.ddot(x, self.w)\n",
    "                #score = x.dot(self.w)\n",
    "                self.loss.append(max(0,1 - y*score))\n",
    "                \n",
    "                # If there was an error, update the weights by the hinge loss algorithm\n",
    "                if y*score < 1:\n",
    "                    blas.daxpy(x, self.w, a = eta*y)\n",
    "                    #self.w = blas.dscal((1 - blas.dscal(eta, self.C)), self.w) + blas.dscal(blas.dscal(eta, y), x)\n",
    "                    #self.w = (1-eta*self.C)*self.w + (eta*y)*x\n",
    "                else:\n",
    "                    blas.dscal((1 - blas.dscal(eta, self.C)), self.w)\n",
    "                    #self.w = (1-eta*self.C)*self.w\n",
    "                \n",
    "                \n",
    "                \n",
    "            avg_loss = sum(self.loss)/len(self.loss)\n",
    "            print(f\"Iteration {i}, average loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d205b3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, average loss: 1.136814290620031\n",
      "Iteration 1, average loss: 0.7451121646326183\n",
      "Iteration 2, average loss: 0.6047834514729149\n",
      "Iteration 3, average loss: 0.5312201619033844\n",
      "Iteration 4, average loss: 0.485437106190431\n",
      "Iteration 5, average loss: 0.4542009231774633\n",
      "Iteration 6, average loss: 0.4313507503206367\n",
      "Iteration 7, average loss: 0.4138675711727284\n",
      "Iteration 8, average loss: 0.400043071863277\n",
      "Iteration 9, average loss: 0.38879098722244476\n",
      "Iteration 10, average loss: 0.37947640483414175\n",
      "Iteration 11, average loss: 0.3716506572774833\n",
      "Iteration 12, average loss: 0.364949217477231\n",
      "Iteration 13, average loss: 0.35913254324489224\n",
      "Iteration 14, average loss: 0.35405525249946584\n",
      "Iteration 15, average loss: 0.34956794242756417\n",
      "Iteration 16, average loss: 0.3455755437710357\n",
      "Iteration 17, average loss: 0.3419970180264606\n",
      "Iteration 18, average loss: 0.3387729686268025\n",
      "Iteration 19, average loss: 0.3358575205562579\n",
      "Training time: 7.92 sec.\n",
      "Accuracy: 0.8326.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline_fsvc = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our LinearSVC, not sklearn.linear_model.LinearSVC\n",
    "        FastLinearSVC(n_iter=20, C=1/len(Xtrain_svc))  \n",
    "    )\n",
    "\n",
    "# Train the classifier.\n",
    "t0_fsvc = time.time()\n",
    "pipeline_fsvc.fit(Xtrain_svc, Ytrain_svc)\n",
    "t1_fsvc = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1_fsvc-t0_fsvc))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess_fsvc = pipeline_fsvc.predict(Xtest_svc)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest_svc, Yguess_svc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d228d86",
   "metadata": {},
   "source": [
    "By using BLAS, we speed up the model operations.The running time was reduced from 8.78s to 7.92s, which is around 10% more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b25ab",
   "metadata": {},
   "source": [
    "###  (b)Using sparse vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665bbd3",
   "metadata": {},
   "source": [
    "#### Remove the SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5589258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, average loss: 0.8551040106180142\n",
      "Iteration 1, average loss: 0.5695442812563352\n",
      "Iteration 2, average loss: 0.45939591191577017\n",
      "Iteration 3, average loss: 0.3986603357795712\n",
      "Iteration 4, average loss: 0.3597977882010865\n",
      "Iteration 5, average loss: 0.3326215911003023\n",
      "Iteration 6, average loss: 0.31235966955151884\n",
      "Iteration 7, average loss: 0.29662620575027293\n",
      "Iteration 8, average loss: 0.2840651299339518\n",
      "Iteration 9, average loss: 0.2737257159836662\n",
      "Training time: 38.82 sec.\n",
      "Accuracy: 0.8414.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X_svc, Y_svc = read_data('all_sentiment_shuffled.txt')\n",
    "    \n",
    "# Split into training and test parts.\n",
    "Xtrain_svc, Xtest_svc, Ytrain_svc, Ytest_svc = train_test_split(X_svc, Y_svc, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline_svc = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our LinearSVC, not sklearn.linear_model.LinearSVC\n",
    "        LinearSVC(n_iter=10, C=1/len(Xtrain_svc))  \n",
    "    )\n",
    "\n",
    "# Train the classifier.\n",
    "t0_svc = time.time()\n",
    "pipeline_svc.fit(Xtrain_svc, Ytrain_svc)\n",
    "t1_svc = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1_svc-t0_svc))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess_svc = pipeline_svc.predict(Xtest_svc)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest_svc, Yguess_svc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac41ef",
   "metadata": {},
   "source": [
    "#### Add the option ngram_range=(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea4927b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, average loss: 0.5659392939180279\n",
      "Iteration 1, average loss: 0.37366649618075776\n",
      "Iteration 2, average loss: 0.29500332747658253\n",
      "Iteration 3, average loss: 0.2500058832639561\n",
      "Iteration 4, average loss: 0.2203034004202732\n",
      "Iteration 5, average loss: 0.19916315148335478\n",
      "Iteration 6, average loss: 0.18324313363366612\n",
      "Iteration 7, average loss: 0.1707987859029892\n",
      "Iteration 8, average loss: 0.1607331626639251\n",
      "Iteration 9, average loss: 0.15241574560310717\n",
      "Training time: 1272.28 sec.\n",
      "Accuracy: 0.8695.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X_svc, Y_svc = read_data('all_sentiment_shuffled.txt')\n",
    "    \n",
    "# Split into training and test parts.\n",
    "Xtrain_svc, Xtest_svc, Ytrain_svc, Ytest_svc = train_test_split(X_svc, Y_svc, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline_svc = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our LinearSVC, not sklearn.linear_model.LinearSVC\n",
    "        LinearSVC(n_iter=10, C=1/len(Xtrain_svc))  \n",
    "    )\n",
    "\n",
    "# Train the classifier.\n",
    "t0_svc = time.time()\n",
    "pipeline_svc.fit(Xtrain_svc, Ytrain_svc)\n",
    "t1_svc = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1_svc-t0_svc))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess_svc = pipeline_svc.predict(Xtest_svc)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest_svc, Yguess_svc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70576340",
   "metadata": {},
   "source": [
    "### SparsePerceptronSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8f7d1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsePerceptronSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, C=1):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.C = C\n",
    "        self.loss = []\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "        t = 1\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "                t += 1\n",
    "                eta = 1/(self.C*t)\n",
    "                \n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "                self.loss.append(max(0,1 - y*score))\n",
    "                \n",
    "                self.w = (1-eta*self.C)*self.w\n",
    "                # If there was an error, update the weights by the hinge loss algorithm.\n",
    "                if y*score < 1:\n",
    "                    # (This corresponds to self.w += y*x above.)\n",
    "                    add_sparse_to_dense(x, self.w, eta*y)\n",
    "                    \n",
    "            avg_loss = sum(self.loss)/len(self.loss)\n",
    "            print(f\"Iteration {i}, average loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f36d5bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, average loss: 0.7778718047201495\n",
      "Iteration 1, average loss: 0.5309339681184192\n",
      "Iteration 2, average loss: 0.43354922706315596\n",
      "Iteration 3, average loss: 0.3792958536789474\n",
      "Iteration 4, average loss: 0.3443494505213291\n",
      "Iteration 5, average loss: 0.31975586007700996\n",
      "Iteration 6, average loss: 0.30133292374728937\n",
      "Iteration 7, average loss: 0.2869834302416344\n",
      "Iteration 8, average loss: 0.2754734949388451\n",
      "Iteration 9, average loss: 0.26600523259283493\n",
      "Iteration 10, average loss: 0.2580592051796681\n",
      "Iteration 11, average loss: 0.2512963208425407\n",
      "Iteration 12, average loss: 0.24546792762713696\n",
      "Iteration 13, average loss: 0.24036543141102026\n",
      "Iteration 14, average loss: 0.2358741752312414\n",
      "Iteration 15, average loss: 0.23188735923441167\n",
      "Iteration 16, average loss: 0.22831057643680633\n",
      "Iteration 17, average loss: 0.2250968453784514\n",
      "Iteration 18, average loss: 0.22219118095053977\n",
      "Iteration 19, average loss: 0.21954267218167553\n",
      "Training time: 19.44 sec.\n",
      "Accuracy: 0.8410.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X_svc, Y_svc = read_data('all_sentiment_shuffled.txt')\n",
    "    \n",
    "# Split into training and test parts.\n",
    "Xtrain_svc, Xtest_svc, Ytrain_svc, Ytest_svc = train_test_split(X_svc, Y_svc, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline_svc = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our LinearSVC, not sklearn.linear_model.LinearSVC\n",
    "        SparsePerceptronSVC(n_iter=20, C=1/len(Xtrain_svc))  \n",
    "    )\n",
    "\n",
    "# Train the classifier.\n",
    "t0_svc = time.time()\n",
    "pipeline_svc.fit(Xtrain_svc, Ytrain_svc)\n",
    "t1_svc = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1_svc-t0_svc))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess_svc = pipeline_svc.predict(Xtest_svc)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest_svc, Yguess_svc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a501e",
   "metadata": {},
   "source": [
    "We tested remove SelectKBest and add option ngram_range=(1,2) respectively, and found that their runtimes increased significantly. With SparsePerceptronSVC, the runtime decreases significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aceaa07",
   "metadata": {},
   "source": [
    "### (c) Speeding up the scaling operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "69b18268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalingLinearSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, C=1):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.C = C\n",
    "        self.loss = []\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        a = 1.0\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "        t = 1\n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            for x, y in XY:\n",
    "                t += 1\n",
    "                eta = 1/(self.C*t)\n",
    "                \n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = a*sparse_dense_dot(x, self.w)\n",
    "                self.loss.append(max(0,1 - y*score))\n",
    "                \n",
    "                a = (1 - self.C*eta)*a\n",
    "                if a == 0.:\n",
    "                    a = 1e-9\n",
    "                \n",
    "                #self.w = (1-eta*self.C)*self.w\n",
    "                # If there was an error, update the weights by the hinge loss algorithm\n",
    "                if y*score < 1:\n",
    "                    # (This corresponds to self.w += y*x above.)\n",
    "                    add_sparse_to_dense(x, self.w, (eta*y)/a)\n",
    "                    \n",
    "            avg_loss = sum(self.loss)/len(self.loss)\n",
    "            print(f\"Iteration {i}, average loss: {avg_loss}\")\n",
    "        self.w *= a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "685c5098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, average loss: 0.7778718047201497\n",
      "Iteration 1, average loss: 0.5309339681184203\n",
      "Iteration 2, average loss: 0.43354922706315707\n",
      "Iteration 3, average loss: 0.37929585367894847\n",
      "Iteration 4, average loss: 0.34434945052133004\n",
      "Iteration 5, average loss: 0.3197558600770106\n",
      "Iteration 6, average loss: 0.3013329237472898\n",
      "Iteration 7, average loss: 0.28698343024163464\n",
      "Iteration 8, average loss: 0.27547349493884493\n",
      "Iteration 9, average loss: 0.26600523259283465\n",
      "Iteration 10, average loss: 0.25805920517966785\n",
      "Iteration 11, average loss: 0.25129632084254067\n",
      "Iteration 12, average loss: 0.2454679276271372\n",
      "Iteration 13, average loss: 0.24036543141102076\n",
      "Iteration 14, average loss: 0.23587417523124224\n",
      "Iteration 15, average loss: 0.23188735923441287\n",
      "Iteration 16, average loss: 0.22831057643680744\n",
      "Iteration 17, average loss: 0.22509684537845276\n",
      "Iteration 18, average loss: 0.22219118095054133\n",
      "Iteration 19, average loss: 0.21954267218167697\n",
      "Training time: 11.39 sec.\n",
      "Accuracy: 0.8410.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X_svc, Y_svc = read_data('all_sentiment_shuffled.txt')\n",
    "    \n",
    "# Split into training and test parts.\n",
    "Xtrain_svc, Xtest_svc, Ytrain_svc, Ytest_svc = train_test_split(X_svc, Y_svc, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline_svc = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our LinearSVC, not sklearn.linear_model.LinearSVC\n",
    "        ScalingLinearSVC(n_iter=20, C=1/len(Xtrain_svc))  \n",
    "    )\n",
    "\n",
    "# Train the classifier.\n",
    "t0_svc = time.time()\n",
    "pipeline_svc.fit(Xtrain_svc, Ytrain_svc)\n",
    "t1_svc = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1_svc-t0_svc))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess_svc = pipeline_svc.predict(Xtest_svc)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest_svc, Yguess_svc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a1a63",
   "metadata": {},
   "source": [
    "By comparing results between SparseSVC and ScaledSVC, the model run time reduced by 8.05s, which means increase 41.4% in efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
