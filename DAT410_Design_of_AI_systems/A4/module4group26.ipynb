{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64fc7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "with open('./dat410_europarl/europarl-v7.fr-en.lc.en','r',encoding='utf-8') as f:\n",
    "    words = f.read()\n",
    "    words = re.split('[^\\w+]',words)\n",
    "words = [x for x in words if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf4bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('die', 10521), ('der', 9374), ('und', 7028), ('in', 4175), ('zu', 3169), ('den', 2976), ('wir', 2863), ('daß', 2738), ('ich', 2670), ('das', 2669)]\n",
      "[('apos', 16729), ('de', 14520), ('la', 9746), ('et', 6619), ('l', 6536), ('le', 6174), ('les', 5585), ('à', 5500), ('des', 5232), ('que', 4797)]\n"
     ]
    }
   ],
   "source": [
    "#read files\n",
    "#english\n",
    "de_en_en = open('./dat410_europarl/europarl-v7.de-en.lc.en','r', encoding = \"utf-8\").read()\n",
    "fr_en_en = open('./dat410_europarl/europarl-v7.fr-en.lc.en','r',encoding = \"utf-8\").read()\n",
    "sv_en_en = open('./dat410_europarl/europarl-v7.sv-en.lc.en','r',encoding = \"utf-8\").read()\n",
    "all_en = de_en_en + \" \" + fr_en_en + \" \" + sv_en_en + \" \"\n",
    "\n",
    "#other language\n",
    "de_en_de = open('./dat410_europarl/europarl-v7.de-en.lc.de','r',encoding = \"utf-8\").read()\n",
    "fe_en_fr = open('./dat410_europarl/europarl-v7.fr-en.lc.fr','r',encoding = \"utf-8\").read()\n",
    "sv_en_sv = open('./dat410_europarl/europarl-v7.sv-en.lc.sv','r',encoding = \"utf-8\").read()\n",
    "all_other = de_en_de + \" \" + fe_en_fr + \" \" + sv_en_sv + \" \"\n",
    "\n",
    "#removing special symbols \n",
    "def clean_file(name):\n",
    "    punctuation = string.punctuation\n",
    "    for i in punctuation:\n",
    "        name = name.replace(i,'')  \n",
    "    return name\n",
    "#removing special symbols \n",
    "def clean_file2(textname):\n",
    "    xxx = re.sub(r'[^\\w]', ' ',textname)\n",
    "    return xxx\n",
    "    \n",
    "#find most frequent words\n",
    "def most_frequent_words(file_name, num):\n",
    "    #file = file_name.replace(\",\",\"\")\n",
    "    #file = file.replace(\".\",\"\")\n",
    "    words = Counter(file_name.split()).most_common(num)\n",
    "    print(words)\n",
    "#Germany\n",
    "de_clean = clean_file(de_en_de)\n",
    "de = most_frequent_words(de_clean,10)\n",
    "#French\n",
    "fr_clean = clean_file(fe_en_fr)\n",
    "fr = most_frequent_words(fr_clean,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8824766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of speaker: 2.1350917863018982e-05\n",
      "Probability of zebra: 0.0\n"
     ]
    }
   ],
   "source": [
    " # probability of 'speaker' and 'zebra'\n",
    "all_texts = all_en + \" \" + all_other + \" \"\n",
    "all_texts_clean = clean_file(all_texts)\n",
    "all_texts_counters = Counter(all_texts_clean.split())\n",
    "speaker = all_texts_counters[\"speaker\"]/sum(all_texts_counters.values())\n",
    "zebra = all_texts_counters[\"zebra\"]/sum(all_texts_counters.values())\n",
    "print(f\"Probability of speaker: {speaker}\")\n",
    "print(f\"Probability of zebra: {zebra}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69c31496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigram(lang):\n",
    "    ''''\n",
    "    find the bigram and unigram and count how many times for each to appear in the English text\n",
    "    ''''\n",
    "    bigram = []\n",
    "    unigram = []\n",
    "    for i in range(len(lang)-1):\n",
    "        bigram.append((lang[i],lang[i+1]))\n",
    "    for i in range(len(lang)):\n",
    "        unigram.append(lang[i])\n",
    "        \n",
    "    count_bigram = dict(Counter(bigram))\n",
    "    count_unigram = dict(Counter(unigram))\n",
    "    return bigram, count_bigram, count_unigram # bigram(list),  count_bigram(dict),  count_unigram(dict)\n",
    "\n",
    "def probability(sentence, lang):\n",
    "    '''\n",
    "    calculate the probability of each bigram and unigram of one individual input sentence\n",
    "    '''\n",
    "    bigrams, count_bigrams, count_unigram = find_bigram(lang)\n",
    "    pro_bigram = {} # dict to store the probability of each bigram\n",
    "    sentence_split = sentence.split()\n",
    "    bi_sen = [] # list to store bigrams of the input sentence\n",
    "    prob = 1\n",
    "    for bigram in list(count_bigrams.keys()):\n",
    "        pro_bigram[bigram] = count_bigrams[bigram]/count_unigram[bigram[0]]\n",
    "        \n",
    "    for i in range(len(sentence_split)-1):\n",
    "        bi_sen.append((sentence_split[i],sentence_split[i+1]))\n",
    "        if bi_sen[i] in pro_bigram:\n",
    "            prob *= pro_bigram[bi_sen[i]]\n",
    "        else:\n",
    "            # if the bigram does not exist in the bigrams of the English text, we consider it appearing 0.1 times\n",
    "            prob *= 0.1/sum(count_bigrams.values())\n",
    "    return prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93bd2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the English file and French file\n",
    "En_file = open('./dat410_europarl/europarl-v7.fr-en.lc.en',encoding = 'utf-8').read()\n",
    "En_file = En_file.splitlines()\n",
    "Fr_file = open('./dat410_europarl/europarl-v7.fr-en.lc.fr',encoding = 'utf-8').read()\n",
    "Fr_file = Fr_file.splitlines()\n",
    "En_file\n",
    "sentence_pair = [] # list to store parallel sentence pair\n",
    "for i in range(len(En_file)):\n",
    "    sentence_pair.append([Fr_file[i],En_file[i]])\n",
    "word_pair = []    \n",
    "# split each sentence pair into individual word\n",
    "for i in range(len( sentence_pair)):\n",
    "    En_word = re.split('[^\\w+]', sentence_pair[i][1])\n",
    "    En_word = [x for x in En_word if x]\n",
    "    En_word.append('null')\n",
    "    \n",
    "    Fr_word = re.split('[^\\w+]', sentence_pair[i][0])\n",
    "    Fr_word = [x for x in Fr_word if x]\n",
    "    \n",
    "    word_pair.append([Fr_word,En_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10a970e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_ENtoFR(integ, word_pair):\n",
    "'''\n",
    "translation model to find the alignment\n",
    "'''\n",
    "    # initialization\n",
    "    trans = {}\n",
    "    for i in range(len(word_pair)):\n",
    "        for en_word in word_pair[i][1]:\n",
    "            for fr_word in word_pair[i][0]:\n",
    "                trans[en_word,fr_word] = random.random()\n",
    "\n",
    "    trans_total = {}\n",
    "    delta = {}\n",
    "    for t in range(integ):\n",
    "        soft_cnt = {}\n",
    "        soft_cntuni = {}\n",
    "        for k in range(len(word_pair)):\n",
    "            for fr_word in word_pair[k][0]:\n",
    "                soft_cntuni[fr_word] = 0\n",
    "                for en_word in word_pair[k][1]:\n",
    "                    soft_cnt[(en_word,fr_word)] = 0\n",
    "\n",
    "        for k in range(len(word_pair)):\n",
    "            for en_word in word_pair[k][1]:\n",
    "                trans_total[en_word] = 0\n",
    "                for fr_word in word_pair[k][0]:\n",
    "                    trans_total[en_word]+= trans[(en_word,fr_word)]\n",
    "                for fr_word in word_pair[k][0]:\n",
    "                    delta[(en_word,fr_word)] = trans[(en_word,fr_word)]/trans_total[en_word]\n",
    "                    soft_cnt[(en_word,fr_word)] +=  delta[(en_word,fr_word)]\n",
    "                    soft_cntuni[fr_word] += delta[(en_word,fr_word)]\n",
    "\n",
    "        for k in range(len(word_pair)):\n",
    "            for en_word in word_pair[k][1]:\n",
    "                for fr_word in word_pair[k][0]:\n",
    "                    trans[(en_word,fr_word)] = soft_cnt[(en_word,fr_word)]/soft_cntuni[fr_word]\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "165301be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['européennes',\n",
       " 'européen',\n",
       " 'européens',\n",
       " 'union',\n",
       " 'européenne',\n",
       " 'risquées',\n",
       " 'prison',\n",
       " 'pic',\n",
       " 'labour',\n",
       " 'aérienne']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trans_ENword(word,t,word_pair):\n",
    "    '''\n",
    "    function to find the most possible 10 words in French given an English word\n",
    "    '''\n",
    "    trans = EM_ENtoFR(t,word_pair)\n",
    "    word_list = {}\n",
    "    word_topten = []\n",
    "    for pair in trans.keys():\n",
    "        if pair[0] == word:\n",
    "            word_list[pair[1]] = trans[pair]\n",
    "    word_top = sorted(word_list.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    for i in range(10):\n",
    "        word_topten.append(word_top[i][0])\n",
    "    \n",
    "    return word_topten\n",
    "\n",
    "trans_ENword('european',3,word_pair)\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aa39472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_FRword(word,t,word_pair):\n",
    "    '''\n",
    "    find the most possible English word given a French word\n",
    "    '''\n",
    "    trans = EM_ENtoFR(t,word_pair)\n",
    "    word_list = {}\n",
    "    for pair in trans.keys():\n",
    "        if pair[1] == word:\n",
    "            word_list[pair[0]] = trans[pair]\n",
    "    word_top = sorted(word_list.items(), key = lambda x:x[1], reverse = True)\n",
    "    word_top = word_top[0][0]\n",
    "    \n",
    "    return word_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68d79cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people has already been introduced a'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trans_sentence(sentence, t, word_pair, words):\n",
    "    '''\n",
    "    translate the given French sentence\n",
    "    '''\n",
    "    sen_trans = [] # store the words in the input sentence\n",
    "    sen_split = sentence.split() # split the sentence\n",
    "    for word in sen_split:\n",
    "        sen_trans.append(trans_FRword(word,t,word_pair))\n",
    "    if 'null' in sen_trans:\n",
    "        # remove \"null\" in the word list\n",
    "        sen_trans = sen_trans.remove('null')\n",
    "    tmp_list = itertools.permutations(sen_trans)\n",
    "    permu_word = [] # store the permutations of the given word list\n",
    "    for res in tmp_list:\n",
    "        permu_word.append(list(res))\n",
    "    \n",
    "    for sub_sen in permu_word:\n",
    "        # find in what sequence the sentence will have the highest probability\n",
    "        sub_sen = ' '.join(sub_sen)\n",
    "        sen_permu[sub_sen] = probability(sub_sen,words)\n",
    "    sen_top = sorted(sen_permu.items(), key = lambda x:x[1], reverse = True)\n",
    "    return sen_top[0][0]\n",
    "trans_sentence('une pétition a déjà été introduite',20,word_pair,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7efdd6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am a darker'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_sentence('je suis un étudiant',10,word_pair,words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
